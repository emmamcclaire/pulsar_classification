{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees vs Support Vectors\n",
    "\n",
    "Both these classification algorithms are powerful, but one might outperform the other based on the nature of the input data. Here are two illustrative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:20.390148Z",
     "start_time": "2019-05-29T17:35:18.616294Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:23.119019Z",
     "start_time": "2019-05-29T17:35:20.427848Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Diagonal decision boundary\n",
    "\n",
    "We choose a 100 random points between (0,1) in two dimensions.  Classification labels are given as 0 or 1 based on points being above or below the diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:24.089142Z",
     "start_time": "2019-05-29T17:35:23.121800Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)\n",
    "y1 = [1 if i[0]>i[1] else 0 for i in X]\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def visualize(X, y, bdry='diag'):\n",
    "    c = cm.rainbow(np.linspace(0, 1, 2))\n",
    "    plt.scatter([i[0] for i in X], [i[1] for i in X], color=[c[i] for i in y], alpha=.5)\n",
    "    \n",
    "    #Plot the true decision boundary\n",
    "    if bdry == 'diag':\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "    elif bdry == 'quadrant':\n",
    "        plt.plot([0, 1], [0.5, 0.5], 'k--')\n",
    "        plt.plot([0.5, 0.5], [0, 1], 'k--')\n",
    "        \n",
    "    plt.grid(True)\n",
    "    \n",
    "\n",
    "visualize(X, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**QUESTION:** Based on code above, which labels do red and purple have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data are diagonally separable. A linear SVC model can nail this situation, as seen below.\n",
    "\n",
    "First we will define a few helper functions to run some quick simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:24.117594Z",
     "start_time": "2019-05-29T17:35:24.101192Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def quick_test(model, X, y):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)\n",
    "    model.fit(xtrain, ytrain)\n",
    "    return model.score(xtest, ytest)\n",
    "\n",
    "def quick_test_afew_times(model, X, y, n=10):\n",
    "    return np.mean([quick_test(model, X, y) for j in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:24.208923Z",
     "start_time": "2019-05-29T17:35:24.131423Z"
    }
   },
   "outputs": [],
   "source": [
    "linearsvc = LinearSVC()\n",
    "\n",
    "# Do the test 10 times with a LinearSVC and get the average score\n",
    "quick_test_afew_times(linearsvc, X, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **QUESTION:** Why didn't we get 100% accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But these diagonally separable data turn out to be a tough problem for a decision tree. It will easily overfit the data and won't do as well as the SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:24.313448Z",
     "start_time": "2019-05-29T17:35:24.228048Z"
    }
   },
   "outputs": [],
   "source": [
    "decisiontree = DecisionTreeClassifier(max_depth=4)\n",
    "quick_test_afew_times(decisiontree, X, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is likely an improvement, but still not quite as good as the linear SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:26.968815Z",
     "start_time": "2019-05-29T17:35:24.326157Z"
    }
   },
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier(n_estimators=100)\n",
    "quick_test_afew_times(randomforest, X, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at the decision boundary, we see that the decision tree is having a hard time because it can only subdivide the space into rectanuglar regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:31.603752Z",
     "start_time": "2019-05-29T17:35:26.973119Z"
    }
   },
   "outputs": [],
   "source": [
    "decisiontree.fit(X, y1)\n",
    "\n",
    "grid = np.mgrid[0:1.02:0.02, 0:1.02:0.02].reshape(2,-1).T\n",
    "visualize(grid, decisiontree.predict(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the linear SVC makes any _linear_ decision boundary we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:34.334484Z",
     "start_time": "2019-05-29T17:35:31.610163Z"
    }
   },
   "outputs": [],
   "source": [
    "linearsvc.fit(X, y1)\n",
    "\n",
    "grid = np.mgrid[0:1.02:0.02, 0:1.02:0.02].reshape(2,-1).T\n",
    "visualize(grid, linearsvc.predict(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Quadrant decision boundary\n",
    "Now we will try a different categorization. Put an orthogonal axis in the middle of the x- and y-values to make a quadrant-style decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:34.804484Z",
     "start_time": "2019-05-29T17:35:34.338783Z"
    }
   },
   "outputs": [],
   "source": [
    "y2 = [1 if (0.5-i[0])*(0.5-i[1])>0 else 0 for i in X]\n",
    "visualize(X, y2, bdry='quadrant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now linear SVC really struggles because the data isn't linearly seperable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:34.837057Z",
     "start_time": "2019-05-29T17:35:34.807775Z"
    }
   },
   "outputs": [],
   "source": [
    "linearsvc = LinearSVC()\n",
    "quick_test_afew_times(linearsvc, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomal kernel may do a bit better but still won't do well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:34.924630Z",
     "start_time": "2019-05-29T17:35:34.840947Z"
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='poly', gamma='auto')\n",
    "quick_test_afew_times(svc, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees, on the other hand, thrive on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:35.019476Z",
     "start_time": "2019-05-29T17:35:34.955243Z"
    }
   },
   "outputs": [],
   "source": [
    "decisiontree = DecisionTreeClassifier()\n",
    "quick_test_afew_times(decisiontree, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the decision boundaries of each model make it abundantly clear: trees are able to segment according to the quadrants to make local predicitons while the SVCs make global decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:38.117602Z",
     "start_time": "2019-05-29T17:35:35.041102Z"
    }
   },
   "outputs": [],
   "source": [
    "decisiontree.fit(X, y2)\n",
    "\n",
    "grid = np.mgrid[0:1.02:0.02, 0:1.02:0.02].reshape(2,-1).T\n",
    "visualize(grid, decisiontree.predict(grid), bdry='quadrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:41.423156Z",
     "start_time": "2019-05-29T17:35:38.123060Z"
    }
   },
   "outputs": [],
   "source": [
    "linearsvc.fit(X, y2)\n",
    "\n",
    "grid = np.mgrid[0:1.02:0.02, 0:1.02:0.02].reshape(2,-1).T\n",
    "visualize(grid, linearsvc.predict(grid), bdry='quadrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:44.328575Z",
     "start_time": "2019-05-29T17:35:41.427270Z"
    }
   },
   "outputs": [],
   "source": [
    "#Polynomial kernal\n",
    "svc.fit(X, y2)\n",
    "\n",
    "grid = np.mgrid[0:1.02:0.02, 0:1.02:0.02].reshape(2,-1).T\n",
    "visualize(grid, svc.predict(grid), bdry='quadrant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we looked at two simple problems: \n",
    "- The linear SVC and decision tree models each solved a different one perfectly but struggled with the other. \n",
    "- When they struggled, sophistications like non-linear kernels and random forests helped, but still couldn't reach perfection.\n",
    "- **It's easy to throw your data into a bunch of algorithms and see which one does best.  But gaining some intuition for your data and your models is worth the time and effort!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
